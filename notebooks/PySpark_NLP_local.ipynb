{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfAJhrgFo5Tv"
   },
   "source": [
    "# Topic Modelling with PySpark and Spark NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1vt8L5e0mss0"
   },
   "source": [
    "This is the tutorial for topic modelling using [PySpark](https://spark.apache.org/docs/latest/api/python/index.html) and [Spark NLP](https://www.johnsnowlabs.com/) libraries. This code could be seen as a complement of Topic Modelling with PySpark and Spark NLP blog post on medium. You could refer to this blog post for more elaborated explanation on what topic modelling is, how to use Spark NLP for NLP pipelines and perform topic modelling with PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZ9ZF9pQqW2X"
   },
   "source": [
    "The code shows how to install PySpark and Spark NLP libraries and download a Kaggle dataset to Google Collaboratory. It also illustrates how to build the NLP pipeline with Spark NLP and train a topic model with PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BIszqyp8lzSU"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CMxW9ewNmaPX"
   },
   "source": [
    "The initial task is to install PySpark and Spark NLP libraries to our Google Collaboratory and set the data up. We can proceed with the following installation. We experiment with the latest versions of PySpark (2.4.5) and Spark NLP (2.4.5) to date. We also install `nltk` package because we will need it in one of the steps of our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KfgD9jhRrFEF"
   },
   "source": [
    "We are going to use data from [Kaggle](https://www.kaggle.com//). I have chosen [Amazon Musical Instruments Review](https://www.kaggle.com/eswarchandt/amazon-music-reviews) dataset for our topic modelling analysis. The data is not that big to see the benefit of using Spark but it is nice for tutorial purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5D07dyhss2Hk"
   },
   "source": [
    "The data is ready to use. Let's start the Spark session through Spark NLP. If you need a special Spark session setting, you can start Spark session with `SparkSession.builder` from PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My machine has following configuration...\n",
    "- 6 cores with 12vCores\n",
    "- 32GB RAM\n",
    "\n",
    "Spark Standalone server:\n",
    "```\n",
    "cd /opt/softwares/spark-2.4.7-bin-hadoop2.7/\n",
    "\n",
    "export PYSPARK_PYTHON=/opt/envs/ai4e/bin/python\n",
    "export PYSPARK_DRIVER_PYTHON=/opt/envs/ai4e/bin/python\n",
    "\n",
    "sbin/start-all.sh\n",
    "sbin/stop-all.sh\n",
    "```\n",
    "Spark UI: [http://localhost:8080](http://localhost:8080)   \n",
    "Spark Master URL : spark://IMCHLT276:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sparknlp\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_E4acZCg8Ku8"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:92)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:570)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-5cce1ba77c0b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"spark.cores.max\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"12\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"spark.local.dir\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"/opt/tmp/spark-temp/\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m     \u001B[0;34m.\u001B[0m\u001B[0mappName\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"SparkNLP-LDA\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;31m# spark = sparknlp.start()\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36mgetOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    184\u001B[0m                             \u001B[0msparkConf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    185\u001B[0m                         \u001B[0;31m# This SparkContext may be an existing one.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 186\u001B[0;31m                         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msparkConf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    187\u001B[0m                     \u001B[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    188\u001B[0m                     \u001B[0;31m# by all sessions.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36mgetOrCreate\u001B[0;34m(cls, conf)\u001B[0m\n\u001B[1;32m    374\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    375\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 376\u001B[0;31m                 \u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    377\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    378\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    134\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    135\u001B[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001B[0;32m--> 136\u001B[0;31m                           conf, jsc, profiler_cls)\n\u001B[0m\u001B[1;32m    137\u001B[0m         \u001B[0;32mexcept\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    138\u001B[0m             \u001B[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m_do_init\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    197\u001B[0m         \u001B[0;31m# Create the Java SparkContext through Py4J\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 198\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mjsc\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_initialize_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    199\u001B[0m         \u001B[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    200\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_jconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m_initialize_context\u001B[0;34m(self, jconf)\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0mInitialize\u001B[0m \u001B[0mSparkContext\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfunction\u001B[0m \u001B[0mto\u001B[0m \u001B[0mallow\u001B[0m \u001B[0msubclass\u001B[0m \u001B[0mspecific\u001B[0m \u001B[0minitialization\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    314\u001B[0m         \"\"\"\n\u001B[0;32m--> 315\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mJavaSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    316\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    317\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mclassmethod\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1567\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1568\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1569\u001B[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001B[0m\u001B[1;32m   1570\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1571\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m                 raise Py4JJavaError(\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m                 raise Py4JError(\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:92)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:570)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://IMCHLT276:7077\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.cores.max\", \"12\") \\\n",
    "    .config(\"spark.local.dir\", \"/opt/tmp/spark-temp/\") \\\n",
    "    .appName(\"SparkNLP-LDA\") \\\n",
    "    .getOrCreate()\n",
    "# spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IdZPmaFPtwyq"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Twlcj7xFt318"
   },
   "source": [
    "Here we start our topic modelling tutorial. First, we access the downloaded data and read it into Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19uyI_8dAd3j"
   },
   "outputs": [],
   "source": [
    "data_path = '../data/Musical_instruments_reviews.csv'\n",
    "data = spark.read.csv(data_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BohpGAxPuezM"
   },
   "source": [
    "Let's checkout what kind of information is stored in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "x_4_zyfNBlzB",
    "outputId": "26f0b424-91c5-4e79-b3b8-99e30e1ac1ce"
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2yf6V87uk9F"
   },
   "source": [
    "For topic modelling, we need only textual data, thus, we create a new dataframe only with the column of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h71u8qFR7g40"
   },
   "outputs": [],
   "source": [
    "text_col = 'reviewText'\n",
    "review_text = data.select(text_col).filter(F.col(text_col).isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "quLpRWAEu2Gd"
   },
   "source": [
    "The data that we will use further for the analysis looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "cID1bvIuyWgn",
    "outputId": "fb879f7d-0c8d-4c35-dc38-761c11bca5c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                                reviewText|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                          that's just like|\n",
      "|The product does exactly as it should and is quite affordable.I did not realized it was...|\n",
      "|The primary job of this device is to block the breath that would otherwise produce a po...|\n",
      "|Nice windscreen protects my MXL mic and prevents pops. Only thing is that the gooseneck...|\n",
      "|This pop filter is great. It looks and performs like a studio filter. If you're recordi...|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_text.limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9hHCLbrpfSqO"
   },
   "source": [
    "## Spark NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i36XwEh8vL85"
   },
   "source": [
    "Here we start our NLP pipeline for the task of topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pa4UrSFFvizm"
   },
   "source": [
    "### Basic NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELN2q7q-vppo"
   },
   "source": [
    "Let's start with basic NLP pipeline that clears the data and gets lemmatized unigrams. To understand how you can use Spark NLP annotators (estimators and transformers) for NLP pipeline, you can refer to [Spark NLP documentation](https://nlp.johnsnowlabs.com/) or a corresponding blog post on Topic Modelling with PySpark and Spark NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4PjxUquwv83c"
   },
   "source": [
    "We will start with [**DocumentAssembler**](https://nlp.johnsnowlabs.com/docs/en/transformers#documentassembler-getting-data-in) that converts data into Spark NLP annotation format that can be used by Spark NLP annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwOdzQ_PAJi0"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-ebe952cd5311>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msparknlp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbase\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDocumentAssembler\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mdocumentAssembler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDocumentAssembler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m      \u001B[0;34m.\u001B[0m\u001B[0msetInputCol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext_col\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m      \u001B[0;34m.\u001B[0m\u001B[0msetOutputCol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'document'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/pyspark/__init__.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    108\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Method %s forces keyword arguments.\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_input_kwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 110\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    111\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/sparknlp/base.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    146\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mkeyword_only\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    147\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 148\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDocumentAssembler\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclassname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"com.johnsnowlabs.nlp.DocumentAssembler\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    149\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_setDefault\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"document\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcleanupMode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'disabled'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/pyspark/__init__.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    108\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Method %s forces keyword arguments.\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_input_kwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 110\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    111\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/sparknlp/internal.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, classname)\u001B[0m\n\u001B[1;32m     70\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetParams\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_class_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclassname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 72\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_new_java_obj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclassname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     73\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_new_java_obj\u001B[0;34m(java_class, *args)\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[0mjava_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_obj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m         \u001B[0mjava_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0m_py2java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0marg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 69\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mjava_obj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mjava_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     70\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol(text_col) \\\n",
    "     .setOutputCol('document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "facUm7iMwUKZ"
   },
   "source": [
    "Next step is to tokenize data with [**Tokenizer**](https://nlp.johnsnowlabs.com/docs/en/annotators#tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGOLmEzJDCYW"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-0ab6b5f6626b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msparknlp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mannotator\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mTokenizer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mtokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m      \u001B[0;34m.\u001B[0m\u001B[0msetInputCols\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'document'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m      \u001B[0;34m.\u001B[0m\u001B[0msetOutputCol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'tokenized'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/pyspark/__init__.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    108\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Method %s forces keyword arguments.\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_input_kwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 110\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    111\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/sparknlp/annotator.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    170\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mkeyword_only\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 172\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclassname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"com.johnsnowlabs.nlp.annotators.Tokenizer\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    173\u001B[0m         self._setDefault(\n\u001B[1;32m    174\u001B[0m             \u001B[0mtargetPattern\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"\\\\S+\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/pyspark/__init__.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    108\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Method %s forces keyword arguments.\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_input_kwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 110\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    111\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/sparknlp/common.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, classname)\u001B[0m\n\u001B[1;32m    153\u001B[0m         \u001B[0m_internal\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mParamsGettersSetters\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    154\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_class_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclassname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 155\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_new_java_obj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclassname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    156\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_setDefault\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlazyAnnotator\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/envs/ai4e/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_new_java_obj\u001B[0;34m(java_class, *args)\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[0mjava_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_obj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m         \u001B[0mjava_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0m_py2java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0marg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 69\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mjava_obj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mjava_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     70\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['document']) \\\n",
    "     .setOutputCol('tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uA98q-HCxVjb"
   },
   "source": [
    "Further, we clean our data and lowercase it with [**Normalizer**](https://nlp.johnsnowlabs.com/docs/en/annotators#normalizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dTuQv1VXD8xu"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Normalizer\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized') \\\n",
    "     .setLowercase(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80FaYj0axg1r"
   },
   "source": [
    "We are going to lemmatize our text with pretrained lemming model provided by Spark NLP. We can access this model with [**LemmatizerModel**](https://nlp.johnsnowlabs.com/docs/en/models#english---models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "a1lvJInOEEAy",
    "outputId": "1c707936-8490-4154-a028-5aee58ce34a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import LemmatizerModel\n",
    "\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('lemmatized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-fkFtA9yNEc"
   },
   "source": [
    "Spark NLP doesn't provide stop word list, hence, we will use `nltk` package to download stop words for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "Um9iOifSft2t",
    "outputId": "6ea110e1-2f86-4ff2-a715-d22822d7548e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zKDZh-kkybKt"
   },
   "source": [
    "The downloaded list of stop words we will input into [**StopWordsCleaner**](https://nlp.johnsnowlabs.com/docs/en/annotators#stopwordscleaner) that will remove all such words from our lemmatized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJOGb5argbrb"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import StopWordsCleaner\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('unigrams') \\\n",
    "     .setStopWords(eng_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tW87_1g2yr3-"
   },
   "source": [
    "In addition to unigrams, it is good to use n-grams for topic modelling as well since they help to better refine topics. We can get n-grams with [**NGramGenerator**](https://nlp.johnsnowlabs.com/docs/en/annotators#ngramgenerator) in Spark NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsftJKunZO84"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import NGramGenerator\n",
    "\n",
    "ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['lemmatized']) \\\n",
    "    .setOutputCol('ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ymL8JUPrzV0R"
   },
   "source": [
    "We already have our basic NLP pipeline for topic modelling with all necessary steps. However, let's use POS tagger in order to improve our processed data for topic modelling even more with POS tagged data later. For this, we are going to use pretrained POS tagging model provided by Spark NLP. We can access the model with [**PerceptronModel**](https://nlp.johnsnowlabs.com/docs/en/annotators#postagger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "ilOwKkC20Xe5",
    "outputId": "ac663115-1707-4912-f815-a80e109d113a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 4.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import PerceptronModel\n",
    "\n",
    "pos_tagger = PerceptronModel.pretrained('pos_anc') \\\n",
    "    .setInputCols(['document', 'lemmatized']) \\\n",
    "    .setOutputCol('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hl6Xs_zE0Fc8"
   },
   "source": [
    "Now we have everything in Spark NLP annotation format. To be able to process the data further, we need to tranform data with [**Finisher**](https://nlp.johnsnowlabs.com/docs/en/transformers#finisher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g3eYHx-KrDxU"
   },
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher\n",
    "\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['unigrams', 'ngrams', 'pos']) \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DP-kJKHQ0T8e"
   },
   "source": [
    "Now we are ready to input everything into a pipeline. **Pipeline** functionality is accessible with PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oo4ixwfsrMPh"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "     .setStages([documentAssembler,                  \n",
    "                 tokenizer,\n",
    "                 normalizer,                  \n",
    "                 lemmatizer,                  \n",
    "                 stopwords_cleaner, \n",
    "                 pos_tagger,\n",
    "                 ngrammer,  \n",
    "                 finisher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yXiHvEE0gYx"
   },
   "source": [
    "First, we will fit all our estimators and then, transform the data with trained models and transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCkceTwhrOf0"
   },
   "outputs": [],
   "source": [
    "processed_review = pipeline.fit(review_text).transform(review_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t7ovKGdr0p6J"
   },
   "source": [
    "Let's look at the data we finally get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "D-vuMJZD9Lwc",
    "outputId": "a605b4ee-fbf9-4dd6-93fe-d1489adcbca7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|          reviewText|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|    that's just like|              [like]|[that, just, like...|        [IN, RB, IN]|\n",
      "|The product does ...|[product, exactly...|[the, product, do...|[DT, NN, VBP, RB,...|\n",
      "|The primary job o...|[primary, job, de...|[the, primary, jo...|[DT, JJ, NN, IN, ...|\n",
      "|Nice windscreen p...|[nice, windscreen...|[nice, windscreen...|[JJ, NN, NN, NNP,...|\n",
      "|This pop filter i...|[pop, filter, gre...|[this, pop, filte...|[DT, NN, NN, VB, ...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_review.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Oxvz01vfaq6"
   },
   "source": [
    "### Extended NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tNShG0il00g6"
   },
   "source": [
    "Up to now, we have our data in a form of unigrams that are lemmatized, with no stop words in there. I think it is a good idea to incorporate n-grams into our NLP pipeline. We obtained n-grams as one step of our pipeline but now n-grams are messy and have a lot of questionable combinations in there. To tackle this problem, let's filter out strange combinations of words in n-grams based on their POS tags. We can imagine a list of viable combinations like ADJ + NOUN so let's restrict our POS combinations in n-grams to this list. Plus, we can also exclude some POS tags from our unigrams to ensure that we don't use functional words for topic modelling (they can be partially covered by stop words but probably not fully).\n",
    "\n",
    "Doing this POS-based filtering will significantly reduce the vocabulary size for topic modelling which will speed up the whole processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-FnnKmIf2Ssc"
   },
   "source": [
    "Let's start this processing. First, we need join all our POS tags obtained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WbPpOs8ndGE9"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "udf_join_arr = F.udf(lambda x: ' '.join(x), T.StringType())\n",
    "processed_review  = processed_review.withColumn('finished_pos', udf_join_arr(F.col('finished_pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IzSryqMc2an7"
   },
   "source": [
    "Then we start another Spark NLP pipeline in order to get POS tag n-grams that correspond to word n-grams. We start with convertation into Spark NLP annotation format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RrKEpmfoba6M"
   },
   "outputs": [],
   "source": [
    "pos_documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('finished_pos') \\\n",
    "     .setOutputCol('pos_document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "smZ6-1dz2uf3"
   },
   "source": [
    "Then, we tokenize our POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyZdZ7S8aGZ9"
   },
   "outputs": [],
   "source": [
    "pos_tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['pos_document']) \\\n",
    "     .setOutputCol('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8kJmdm52w-M"
   },
   "source": [
    "And generate n-grams from them in the same way we did that for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9QnL0zwrW_w-"
   },
   "outputs": [],
   "source": [
    "pos_ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['pos']) \\\n",
    "    .setOutputCol('pos_ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vG3UTh0N23Yf"
   },
   "source": [
    "Lastly, we are ready to get POS tags ngrams with **Finisher**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CiUGFlMHZqIZ"
   },
   "outputs": [],
   "source": [
    "pos_finisher = Finisher() \\\n",
    "     .setInputCols(['pos', 'pos_ngrams']) \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oSQ1XeNZ2--r"
   },
   "source": [
    "We create this new Spark NLP pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-AufeTzbYwM"
   },
   "outputs": [],
   "source": [
    "pos_pipeline = Pipeline() \\\n",
    "     .setStages([pos_documentAssembler,                  \n",
    "                 pos_tokenizer,\n",
    "                 pos_ngrammer,  \n",
    "                 pos_finisher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bxQJcpiA3EBg"
   },
   "source": [
    "... and again fit it and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0kfpx2azbmJI"
   },
   "outputs": [],
   "source": [
    "processed_review = pos_pipeline.fit(processed_review).transform(processed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUbDHzkZ3JzH"
   },
   "source": [
    "Let's look what kind of data we have to operate with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "x0dBA85zf3Sd",
    "outputId": "0b6b9286-32b7-43bc-83eb-aa7d46588f93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reviewText',\n",
       " 'finished_unigrams',\n",
       " 'finished_ngrams',\n",
       " 'finished_pos',\n",
       " 'finished_pos_ngrams']"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_review.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U5VXa6ir3Q97"
   },
   "source": [
    "And these are our word n-grams with their corresponding pos n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "dPRHCY32eKAH",
    "outputId": "ed75f929-a561-4dc4-c480-7e90e1a0bac9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|     finished_ngrams| finished_pos_ngrams|\n",
      "+--------------------+--------------------+\n",
      "|[that, just, like...|[IN, RB, IN, IN_R...|\n",
      "|[the, product, do...|[DT, NN, VBP, RB,...|\n",
      "|[the, primary, jo...|[DT, JJ, NN, IN, ...|\n",
      "|[nice, windscreen...|[JJ, NN, NN, NNP,...|\n",
      "|[this, pop, filte...|[DT, NN, NN, VB, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_review.select('finished_ngrams', 'finished_pos_ngrams').limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XCXs_3yNfXbG"
   },
   "source": [
    "Now we are ready to filter out not useful for topic modelling analysis POS tags from our data. Let's create the function that does it for unigrams first. We create the custom Python function and then transform it to PySpark UDF to be used on Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YhhKK97NfEbd"
   },
   "outputs": [],
   "source": [
    "def filter_pos(words, pos_tags):\n",
    "    return [word for word, pos in zip(words, pos_tags) \n",
    "            if pos in ['JJ', 'NN', 'NNS', 'VB', 'VBP']]\n",
    "\n",
    "udf_filter_pos = F.udf(filter_pos, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k_hy63My3rrV"
   },
   "source": [
    "Then, we apply this function on columns with unigrams and their POS tags to get filtered unigrams in a separate dataframe column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rEx5CHyqfWeG"
   },
   "outputs": [],
   "source": [
    "processed_review = processed_review.withColumn('filtered_unigrams',\n",
    "                                               udf_filter_pos(F.col('finished_unigrams'), \n",
    "                                                              F.col('finished_pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yh1XVgPL4K6G"
   },
   "source": [
    "That is how our filtered unigrams look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "L22iUDgWgMVd",
    "outputId": "a39d6f9d-fb38-4b96-c97c-7e8d6ce35e6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                         filtered_unigrams|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                                        []|\n",
      "|[exactly, quite, even, expectedas, add, one, carry, small, hint, grape, buy, put, next,...|\n",
      "|[job, device, would, otherwise, pop, allow, reduction, high, frequency, cloth, block, l...|\n",
      "|[nice, windscreen, protect, mic, prevent, thing, gooseneck, able, hold, require, carefu...|\n",
      "|               [filter, great, look, perform, studio, youre, eliminate, pop, record, sing]|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_review.select('filtered_unigrams').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GEDG4jfD4Rux"
   },
   "source": [
    "It is time to filter out improper POS combinations of n-grams. We create the custom function in the same manner as before. Since we deal with bi- and trigrams, we need to restrict tags for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJd-nLnxgd1g"
   },
   "outputs": [],
   "source": [
    "def filter_pos_combs(words, pos_tags):\n",
    "    return [word for word, pos in zip(words, pos_tags) \n",
    "            if (len(pos.split('_')) == 2 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS']) \\\n",
    "            or (len(pos.split('_')) == 3 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                  pos.split('_')[2] in ['NN', 'NNS'])]\n",
    "    \n",
    "udf_filter_pos_combs = F.udf(filter_pos_combs, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Zjj8b9v425f"
   },
   "source": [
    "And we call the function on word and POS n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QH1gNrz6giPU"
   },
   "outputs": [],
   "source": [
    "processed_review = processed_review.withColumn('filtered_ngrams',\n",
    "                                               udf_filter_pos_combs(F.col('finished_ngrams'),\n",
    "                                                                    F.col('finished_pos_ngrams')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_kA7epRY5HYy"
   },
   "source": [
    "Below is what we get after filtering for n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "gcXY0eMChtTw",
    "outputId": "5a9626e9-19d5-4685-8dc5-47d5e372599d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                           filtered_ngrams|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                                        []|\n",
      "|[be_double, double_screen, add_bonus, small_hint, old_grape, grape_candy, cannot_stop, ...|\n",
      "|[primary_job, pop_sound, noticeable_reduction, high_frequency, double_cloth, cloth_filt...|\n",
      "|[nice_windscreen, windscreen_protect, mxl_mic, prevent_pop, require_careful, careful_po...|\n",
      "|             [pop_filter, be_great, studio_filter, youre_record, record_vocal, get_record]|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_review.select('filtered_ngrams').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ciCceJZH5NmN"
   },
   "source": [
    "Now we have unigrams and n-grams stored in different columns in the dataframe. Let's combine them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GVNl50pI7U1S"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "processed_review = processed_review.withColumn('final', \n",
    "                                               concat(F.col('filtered_unigrams'), \n",
    "                                                      F.col('filtered_ngrams')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jxNhhjb5ZOt"
   },
   "source": [
    "And this is our final look of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "k5HOIIzniFzz",
    "outputId": "5776a696-82b3-4ab0-b445-b8c07cfdc20b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                                     final|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                                        []|\n",
      "|[exactly, quite, even, expectedas, add, one, carry, small, hint, grape, buy, put, next,...|\n",
      "|[job, device, would, otherwise, pop, allow, reduction, high, frequency, cloth, block, l...|\n",
      "|[nice, windscreen, protect, mic, prevent, thing, gooseneck, able, hold, require, carefu...|\n",
      "|[filter, great, look, perform, studio, youre, eliminate, pop, record, sing, pop_filter,...|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_review.select('final').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YkTzRKEzaYLM"
   },
   "source": [
    "There are additional ways how we could provide cleaner data with Spark NLP for topic modelling analysis. For example: \n",
    "\n",
    "* You might want to incorporate [**SentenceDetector**](https://nlp.johnsnowlabs.com/docs/en/annotators#sentencedetector) in order to ignore n-grams on the borders of sentences since tokenization in Spark NLP does not account for sentence borders. \n",
    "\n",
    "* [**DependencyParser**](https://nlp.johnsnowlabs.com/docs/en/annotators#dependency-parsers) also could be used to provide more meaningful n-grams, namely the ones with dependency relation.\n",
    "\n",
    "* Spell checker also could be incorporated at the early steps of the NLP pipeline for less noisy results. There are various options in Spark NLP such as [**NorvigSpellChecker**](https://nlp.johnsnowlabs.com/docs/en/annotators#norvig-spellchecker) and [**SymmetricSpellChecker**](https://nlp.johnsnowlabs.com/docs/en/annotators#symmetric-spellchecker).\n",
    "\n",
    "However, in this tutorial we will omit these options since they probably will not bring significant improvements fot topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-OTK4DVHxcR0"
   },
   "source": [
    "## Vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OP4HlvFR5jXp"
   },
   "source": [
    "Now we are set to vectorization of our data. First, we will proceed with **TF** (*term frequency*) vectorization with **CountVectorizer** in PySpark. We fit tf dictionary and then transform the data to vectors of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqGT4ss7r_w2"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tfizer = CountVectorizer(inputCol='final', outputCol='tf_features')\n",
    "tf_model = tfizer.fit(processed_review)\n",
    "tf_result = tf_model.transform(processed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "217ZJrT76B2h"
   },
   "source": [
    "After we get TF results, we can account for words that are frequent for all the documents. We can use **IDF** (*inverse document frequency*) to lower score of such words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ox3dEUoyx2Ss"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezlCICGwzvQu"
   },
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kelkPspY6okR"
   },
   "source": [
    "Finally, we are ready to model topics in our data with [**LDA**](https://spark.apache.org/docs/2.2.0/ml-clustering.html#latent-dirichlet-allocation-lda) (*Latent Dirichlet Allocation*). To use the algorithm, we have to provide the number of topics we presume our data contains and the number of iterations for the LDA algorithm. Then, we initialize the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ID3lf4GjzxJq"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "num_topics = 6\n",
    "max_iter = 10\n",
    "\n",
    "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features')\n",
    "lda_model = lda.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DslMpvbT7GXQ"
   },
   "source": [
    "To be able to see words that characterize the defined topics, we need to convert word ids into actual words with the custom function. This function will again be converted to PySpark UDF to be used on our topic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OxbhwXnQ0dEa"
   },
   "outputs": [],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "\n",
    "def get_words(token_list):\n",
    "     return [vocab[token_id] for token_id in token_list]\n",
    "       \n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GllCV-hw7fgL"
   },
   "source": [
    "Let's define the number of top words per topic we would like to see and extract the words with our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "Fhsk4J5p0lOm",
    "outputId": "e8e5edc3-8b63-47e1-dfcb-9aba290208e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------+\n",
      "|topic|                                                                          topicWords|\n",
      "+-----+------------------------------------------------------------------------------------+\n",
      "|    0|             [pick, bow, shockmount, store, pm, use, mic, tortex_pick, nice, guitar]|\n",
      "|    1|                  [pedal, amp, great, sound, tuner, good, cable, use, effect, price]|\n",
      "|    2|                     [string, use, guitar, sound, one, strap, get, well, work, good]|\n",
      "|    3|       [mic, use, guitar, good, word, great, quality, word_word, work, good_quality]|\n",
      "|    4|                    [use, string, pick, pedal, one, good, buy, guitar, sound, great]|\n",
      "|    5|[guitar, harmonica, yamaha, use, volume_pedal, string, record, tuner, battery, work]|\n",
      "+-----+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 10\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CcwVxupn7wKe"
   },
   "source": [
    "And that's it! We are done with topic modelling pipeline on review data. I hope this tutorial was somehow helpful for you. \n",
    "\n",
    "Good luck with your own experiments!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOozdhjd8hlmlxdkH+C2Shf",
   "collapsed_sections": [
    "BIszqyp8lzSU",
    "IdZPmaFPtwyq",
    "9hHCLbrpfSqO",
    "Pa4UrSFFvizm",
    "_Oxvz01vfaq6",
    "-OTK4DVHxcR0",
    "ezlCICGwzvQu"
   ],
   "include_colab_link": true,
   "name": "Topic Modelling with PySpark and Spark NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}